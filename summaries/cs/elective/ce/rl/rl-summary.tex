\documentclass[english, notodo]{fdsummary}

\usepackage{amsthm}
\let\definition\undefined
\let\theorem\undefined
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{plain}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{postulate}{Postulate}
\newtheorem{remark}{Remark}
\usepackage{acronym}
\usepackage{xfrac}
\usepackage{multirow}

\let\realsfrac=\sfrac
\renewcommand{\sfrac}[2]{\realsfrac{#1}{#2}}  % Just to make the command known to TeXStudio.

\newcommand{\transposed}{{\!\top\!}}
\newcommand{\MSVE}{\overline{\mathit{VE}}}
\renewcommand{\H}{\mathbb{H}}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\clip}{clip}

\newacro{AI}{artificial intelligence}
\newacro{RL}{reinforcement learning}
\newacro{RBF}{radial basis function}
\newacro{ML}{machine learning}
\newacro{NN}{neural network}
\newacro{MDP}{Markov decision process}  \newacroplural{MDP}{Markov decision processes}
\newacro{MRP}{Markov reward process}  \newacroplural{MRP}{Markov reward processes}
\newacro{DP}{dynamic programming}
\newacro{PI}{policy iteration}
\newacro{VI}{value iteration}
\newacro{MC}{Monte-Carlo}
\newacro{TD}{temporal difference}
\newacro{GLIE}{greedy in the limit of infinite exploration}
\newacro{SARSA}{state, action, reward, state, action}  \acused{SARSA}
\newacro{DQN}{deep Q-network}
\newacro{A2C}{advantage actor-critic}
\newacro{TRPO}{trust-region actor-critic}
\newacro{PPO}{proximal policy optimization}
\newacro{DDPG}{deep deterministic policy gradient}
\newacro{TD3}{twin delayed \acs{DDPG}}
\newacro{SAC}{soft actor-critic}
\newacro{MSVE}{mean squared value error}
\newacro{SGD}{stochastic gradient descent}
\newacro{LSTD}{least-squares \ac{TD}}
\newacro{LSPI}{least-squares \ac{PI}}
\newacro{LSTDQ}{\ac{LSTD} for action-value functions}  \acused{LSTDQ}
\newacro{PG}{policy gradient}
\newacro{LSFD}{least-squares-based finite difference}
\newacro{POMDP}{partially observable \ac{MDP}}
\newacro{GPOMDP}{gradient of \ac{POMDP}}  \acused{GPOMDP}
\newacro{FIM}{Fisher information matrix}
\newacro{KL}{Kullback-Leibler}
\newacro{KKT}{Karush-Kuhn-Tucker}
\newacro{MSE}{mean squared error}
\newacro{SOTA}{state of the art}
\newacro{CFA}{compatible function approximation}
\newacro{eNAC}{episodic natural actor-critic}
\newacro{NG}{natural gradient}
\newacro{DDQN}{double \ac{DQN}}
\newacro{KDE}{kernel density estimation}
\newacro{PGT}{policy gradient theorem}
\newacro{GAE}{generalized advantage estimation}
\newacro{CG}{conjugate gradient}
\newacro{POMCP}{partially observable \ac{MC} planning}
\newacro{SMDP}{semi-Markov decision process}
\newacro{HAM}{hierarchy of abstract machines}
\newacro{PILCO}{probabilistic inference for learning control}
\newacro{GPS}{guided policy search}
\newacro{MBRL}{model-based \ac{RL}}

\begin{document}
	\maketitle
	\listoftodos
	\tableofcontents
	\listoffigures
	\listoftables
	\listofalgorithms

	\include{content}

	\appendix
	\include{appendix}
\end{document}
